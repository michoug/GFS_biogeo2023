

dirs<-list.dirs(full.names=F, recursive = FALSE)

for(i in dirs){
  wd=paste0("path/to/dir",i)
  # the OTU table file (Tab delimited txt file)
  
  com.file="Asv_table.txt"
  
  # the phylogenetic tree file
  tree.file="tree.nwk"
  
  # the classification (taxonomy) information
  clas.file="classification.txt"
  
  
  # the folder to save the output. please change to a new folder even if you are just testing the example data.
  save.wd=paste0(wd,"/out") 
  if(!dir.exists(save.wd)){dir.create(save.wd)}
  
  # 2 # key parameter setting
  prefix=i  # prefix of the output file names. usually use a project ID.
  rand.time=1000  # randomization time, 1000 is usually enough. For example test, you may set as 100 or less to save time.
  nworker=12 # nworker is thread number for parallel computing, which depends on the CPU core number of your computer.
  memory.G=70 # to set the memory size as you need (but should be less than the available space in your hard disk), so that calculation of large tree will not be limited by physical memory. unit is Gb.
  
  # 3 # load R packages and data
  library(iCAMP)
  library(ape)
  setwd(wd)
  comm=t(read.table(com.file, header = TRUE, sep = "", row.names = 1,
                    as.is = TRUE, stringsAsFactors = FALSE, comment.char = "",
                    check.names = FALSE))
  tree=read.tree(file = tree.file)
  clas=read.table(clas.file, header = TRUE, sep = "", row.names = 1,
                  as.is = TRUE, stringsAsFactors = FALSE, comment.char = "",
                  check.names = FALSE)
  
  # 5 # match OTU IDs in OTU table and tree file
  spid.check=match.name(cn.list=list(comm=comm),rn.list=list(clas=clas),tree.list=list(tree=tree))
  # for the example data, the output should be "All match very well".
  # for your data files, if you have not matched the IDs before, the unmatched OTUs will be removed.
  comm=spid.check$comm
  clas=spid.check$clas
  tree=spid.check$tree
  
  # 6 # calculate pairwise phylogenetic distance matrix.
  # since microbial community data usually has a large number of species (OTUs or ASVs), we use "big.matrix" in R package "bigmemory" to handle the large phylogenetic distance matrix. 
  setwd(save.wd)
  if(!file.exists("pd.desc")) 
  {
    pd.big=iCAMP::pdist.big(tree = tree, wd=save.wd, nworker = nworker, memory.G = memory.G)
    # output files:
    # path.rda: a R object to list all the nodes and  edge lengthes from root to every tip. saved in R data format. an intermediate output when claculating phylogenetic distance matrix.
    # pd.bin: BIN file (backingfile) generated by function big.matrix in R package bigmemory. This is the big matrix storing pairwise phylogenetic distance values. By using this bigmemory format file, we will not need memory but hard disk when calling big matrix for calculation.
    # pd.desc: the DESC file (descriptorfile) to hold the backingfile (pd.bin) description.
    # pd.taxon.name.csv: comma delimited csv file storing the IDs of tree tips (OTUs), serving as the row/column names of the big phylogenetic distance matrix.
  }else{
    # if you already calculated the phylogenetic distance matrix in a previous run
    pd.big=list()
    pd.big$tip.label=read.csv(paste0(save.wd,"/pd.taxon.name.csv"),row.names = 1,stringsAsFactors = FALSE)[,1]
    pd.big$pd.wd=save.wd
    pd.big$pd.file="pd.desc"
    pd.big$pd.name.file="pd.taxon.name.csv"
  }
  
  
  ####################
  # iCAMP analysis without omitting small bins.
  # commonly use # set sig.index as Confidence instead of SES.RC (betaNRI/NTI + RCbray)
  
  bin.size.limit = 48 # For real data, usually use a proper number according to phylogenetic signal test or try some settings then choose the reasonable stochasticity level. our experience is 12, or 24, or 48. but for this example dataset which is too small, have to use 5.
  sig.index="Confidence" # see other options in help document of icamp.big.
  icres=iCAMP::icamp.big(comm=comm, pd.desc = pd.big$pd.file, pd.spname=pd.big$tip.label,
                         pd.wd = pd.big$pd.wd, rand = rand.time, tree=tree,
                         prefix = prefix, ds = 0.2, pd.cut = NA, sp.check = TRUE,
                         phylo.rand.scale = "within.bin", taxa.rand.scale = "across.all",
                         phylo.metric = "bMPD", sig.index=sig.index, bin.size.limit = bin.size.limit, 
                         nworker = nworker, memory.G = memory.G, rtree.save = FALSE, detail.save = TRUE, 
                         qp.save = FALSE, detail.null = FALSE, ignore.zero = TRUE, output.wd = save.wd, 
                         correct.special = TRUE, unit.sum = rowSums(comm), special.method = "depend",
                         ses.cut = 1.96, rc.cut = 0.95, conf.cut=0.975, omit.option = "no",meta.ab = NULL)
  
  bins<-icres$detail$taxabin$sp.bin
  res1<-icres$CbMPDiCBraya
  write.csv(res1, paste0(i,".res.csv"))
  write.csv(bins, paste0(i,".bins.csv"))
  
  icbin=iCAMP::icamp.bins(icamp.detail = icres$detail,
                          clas=clas,silent=FALSE, boot = FALSE,
                          rand.time = rand.time,between.group = FALSE)
  save(icbin,file = paste0(prefix,".iCAMP.Summary.rda")) # just to archive the result. rda file is automatically compressed, and easy to load into R.
  write.csv(icbin$Pt,file = paste0(prefix,".ProcessImportance_EachGroup.csv"),row.names = FALSE)
  write.csv(icbin$Ptk,file = paste0(prefix,".ProcessImportance_EachBin_EachGroup.csv"),row.names = FALSE)
  write.csv(icbin$Ptuv,file = paste0(prefix,".ProcessImportance_EachTurnover.csv"),row.names = FALSE)
  write.csv(icbin$BPtk,file = paste0(prefix,".BinContributeToProcess_EachGroup.csv"),row.names = FALSE)
  write.csv(data.frame(ID=rownames(icbin$Class.Bin),icbin$Class.Bin,stringsAsFactors = FALSE),
            file = paste0(prefix,".Taxon_Bin.csv"),row.names = FALSE)
  write.csv(icbin$Bin.TopClass,file = paste0(prefix,".Bin_TopTaxon.csv"),row.names = FALSE)
}
